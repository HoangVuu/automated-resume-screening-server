[{
        "description_text": "<div><div><p>AbleTo is the leading provider of high quality, technology-enabled behavioral health care. AbleTo believes that everyone deserves access to high-quality care, and offers a suite of technology-enabled services to empower people to lead better lives through better mental health. A proprietary platform connects individuals with AbleTo highly trained licensed providers who deliver weekly sessions by phone or video supported by an integrated digital experience. Members also have access to mental health coaches, and digital support programs. Clinical best practices are leveraged across all services to ensure individuals are getting the care they need. Our outcomes-focused care is proven to improve depression and anxiety by over 50% on average, as demonstrated in several peer-reviewed studies. For patients with high-cost medical conditions, our clinically-tailored treatment is proven to reduce medical costs with care that empowers the patient to address their health needs. AbleTo partners with payers to make high-quality care accessible, affordable and convenient for millions of people to get the treatment they need.</p></div><p> Data is integral to nearly everything that we do at AbleTo, and as a result the data team plays a key role at our company. You should be excited to work with data, have the curiosity to dive deeply into issues, and feel empowered to make a meaningful impact at a mission-driven company. We are a team committed to agile value delivery and solid engineering principles, as well as continuously improving our craft. If you love shipping software that delivers deep and meaningful impact to people's mental and behavioral health, join us!</p> <ul> <li>Build and maintain batch and real-time ETL pipelines in a Google Cloud Platform architecture (BigQuery, Dataproc, Firestore, etc.)</li> <li>Identify code quality issues and implement tests to improve future processes.</li> <li>Implement data integrity tests to ensure we are ingesting accurate data.</li> <li>Translate business requirements into actionable data tasks.</li> <li>Partner with business users to understand their needs, come up with end to end solutions, and communicate the results back to the users.</li> <li>Implement high-quality test-driven code.</li> </ul> </div>",
        "requirement_text": "<ul> <li>1+ years experience coding in Python.</li> <li>Experience working with structured and NOSQL databases.</li> <li>Familiarity with structuring and writing ETLs.</li> <li>Experience working with Airflow and Bigquery is a big plus.</li> <li>Experience working with a multitude of stakeholders is a big plus.</li> </ul>",
        "job_title": "Data Engineer I"
    },
    {
        "description_text": "<div><div><b>About Datadog:</b></div> <p> At Datadog, we’re on a mission to build the best monitoring platform in the world. We operate at high scale—trillions of data points per day—and high availability, providing always-on alerting, visualization, and tracing for our customers' infrastructure and applications around the globe.<br> </p> <p><b> The team:</b></p> <p> We are building a first-class Internal Analytics team composed of Data Engineers and Data Analysts. If you’re excited to work on a fast-moving team with cutting-edge open-source data collection, transformation and analysis tools, we want to meet you.</p><br> <p></p> <p><b> You will:</b></p> <ul> <li>Collect data from a wide range of sources: AWS S3, Redshift, PostgreSQL, and various APIs</li> <li>Build data ETL pipelines using Spark, Luigi and other open-source technologies, with programming languages like Scala, Python, and SQL</li> <li>Tune Spark jobs to improve performance</li> <li>Work closely with product managers, designers, and engineers in order to collect the right data that will help them better understand our customers, product usage, or our own operations</li> <li>Work with Data Analysts to build the right analytics reports</li> <li>Have a meaningful impact on many teams at Datadog thanks to data</li> <li>Join a tightly knit team solving hard problems the right way</li> <li>Grow with the company<br> </li> </ul> </div>",
        "requirement_text": "<ul> <li>You are fluent in several programming languages such as Python, R, or Scala</li> <li>You have 2+ years of work experience in building ETL pipelines in production</li> <li>You value code simplicity and performance</li> <li>You have work experience with data storage such as AWS S3, Redshift or similar.</li> <li>Being a SQL expert is a minimum for this position</li> <li>You are fluent with command line</li> <li>You enjoy wrangling huge amounts of data and exploring new data sets</li> <li>You have a natural curiosity and investigative mindset - driven to know “why”.</li> <li>You can explain complex datasets in very clear ways</li> <li>You want to work in a fast, high-growth startup environment and thrive on autonomy</li></ul>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div> <p> Are you excited by the challenges of building up big data infrastructure and results-oriented and providing data solutions to resolve complex data-related problems? We are looking for a talented Data Engineer to join the Data and Analytics team. You will design and own the DDSA data infrastructure and build up robust pipelines. You will have an opportunity to improve efficiency and data quality. You will interface with other technology teams to extract, transform, and load data from a wide variety of data sources using big data technology.</p><p> A successful candidate will have deep knowledge of business intelligence solutions, big data technology, and have the ability to work with various customers to drive and implement data solutions. They will have a passion for data, be a self-starter, comfortable with ambiguity, strong attention to detail. In this role, you will be able to apply your technical skills in developing Data recommendation infrastructure, large databases, recommendation tools, and systems to impact the business.</p></div>",
        "requirement_text": "<ul><li><p>Bachelor's Degree or higher in Computer Sciences or similar</p></li><li><p> Minimum 2 years of Software Industry experience</p></li><li><p> 2+ years of development experience with AWS services Must have EMR or Glue, Data Pipeline or Airflow, S3, Jenkins or other CI/CD</p></li><li><p> 2+ years of extensive working knowledge in spark (pyspark or scala).</p></li><li><p> 1+ years of development experience with SQL queries.</p></li><li><p> 1+ years of development experience with realtime data streams Kafka/Kinesis</p></li><li><p> Proficiency working with structured, semi-structured, and unstructured data sets and real-time streaming data feeds</p></li><li><p> Fluency in SQL, Python, or a similar modeling language.</p></li><li><p> Experience with data warehousing databases Redshift, Oracle, and techniques is preferred</p></li><li><p> Expert level usage with Jenkins, GitHub is preferred</p></li><li><p> Experience in the media industry is a plus</p></li><li><p> Must have the legal right to work in the United States</p></li></ul>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div><p>At Bloomberg, our products are fueled by powerful information. We combine data and context to paint the whole picture for our clients, around the clock – from around the world. In Global Data, we’re responsible for delivering this information through innovative technology - quickly and accurately.<br> </p> <p>We see the impact of our work every day, whether it’s using machine learning pipelines to estimate financial product data, architecting systems that guide decision-making, or finding ways for our colleagues to more easily do their jobs. We get to work with market data to detect anomalies, assess the quality of data feeds to evaluate the accuracy of forecasts, and figure out how to best combine system automation with a human touch, bringing value to our businesses and ultimately our clients.<br> </p> <p>Our Team:</p> <p>Enterprising – and not defined by conventional roles. Our goal is to innovate, redefine, and break boundaries in expanding our modern data business. In Global Data’s Technical Operations, our focus is on data engineering and how data is acquired, processed, validated and stored. We’re constantly innovating to create better, more efficient systems to handle the huge variety of data we acquire and deliver to our clients.<br> </p> </div>",
        "requirement_text": "<ul><li>A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, or equivalent professional work experience in software development, data engineering, data science or information technology</li> <li>2+ years of Python programming and scripting in a production environment</li> <li>2+ years of experience working with restful APIs and data modeling within SQL and NoSQL databases</li> <li>Deep understanding of large-scale, distributed systems</li> <li>Legal authorization to work full-time in the United States without requiring visa sponsorship</li> </ul>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div> <p>We are the world’s learning company with more than 24,000 employees operating in 70 countries. We combine world-class educational content and assessment, powered by services and technology, to enable more effective teaching and personalized learning at scale. We believe that wherever learning flourishes so do people.</p> <p>The Entry level Data Engineer is part of a team responsible for supporting Enterprise Data Warehouse (EDW), custom reporting, and the organization's on-demand data needs. Under the direction of the Manager of EDW Systems, you will participate in the planning, designing, developing, installing, testing, and supporting the complete data management solutions to address ongoing business reporting needs and new opportunities. You will have a plenty of opportunities to learn advanced analytics and machine learning with your experienced peers.</p> <p>Responsibilities</p> <ul><li><p>Responsible for maintaining and enhancing our existing data and analytics platform.</p></li> <li><p> Building required data pipelines for optimal extraction, transformation, and loading of data from various data sources using Cloud and SQL technologies and supporting them.</p></li> <li><p> Triaging the support incidents, investigating, identifying the root cause of the problem, and fixing.</p></li> <li><p> Working with stakeholders, including cross-teams and business users, and assisting them with data-related technical issues.</p></li> <li><p>Identifying, and implementing internal process improvements, including re-designing the applications for greater scalability, optimizing data delivery, and automating manual processes.</p></li> <li><p> Communicate effectively across multiple departments and with stakeholders to review business requirements and propose solutions.</p></li> <li><p> Performing other duties as assigned.</p></li> </ul> </div>",
        "requirement_text": "<div> <ul><li><p>B.S. degree, or equivalent, in Computer Science, Information Systems or related field</p></li> <li><p> 0-2 years of project work experience</p></li> <li><p> Basic knowledge of Python and/or Java programming skills</p></li> <li><p> The ability to learn new technologies quickly</p></li> <li><p> Strong analytical and problem-solving skills</p></li> <li><p> Strong oral and written communication skills (English)</p></li> </ul> <ul><li><p>Data warehousing concepts, BI environment</p></li> <li><p> Dimensional modeling, database structures, and query optimization</p></li> <li><p> GCP Big Data products, including BigQuery, Pub/Sub, Cloud Composer, Dataflow, Cloud Storage, and related cloud technologies</p></li> <li><p> Data pipelines (ETL, ELT) and data wrangling procedures using Python and SQL</p></li> <li><p> Batch and stream processing (including GCP Dataflow/Kafka Streams)</p></li> </ul> </div>",
        "job_title": "Data Engineer, Data Warehouse - ENTRY LEVEL"
    },
    {
        "description_text": "<div><p><b>About May Mobility</b></p> <p> May Mobility is a self-driving technology company working to transform today's mobility landscape, by starting with a niche market–low-speed shuttles for public roadways. Our vehicles are on the streets navigating complex downtown scenarios and transporting thousands of people on their daily commute every week. We are establishing a ground game that will propel us into even larger markets in the future.</p> <p> Based in Ann Arbor, Michigan, our team develops driverless technology to give people more time to laugh with friends, to solve an interesting problem, or to enjoy the world around them. We're hiring people who share our passion for building the future, today.</p> <p><b> Software @ May Mobility</b></p> <p> May Software Engineers are changing how the world moves. Whether they're writing software to communicate with our vehicles, improving tooling for autonomy, automating cloud infrastructure for our data processing, or creating experiences for customers, our software engineers think with a systems level view towards making autonomous vehicles a reality today.</p> <p> Our code base is largely built on Python and C, but we also develop in Javascript, Groovy, HTML, and more. We welcome engineers from a variety of backgrounds as long as they are comfortable working in one of our primary languages or are willing to learn. Because safety and reliability are our top priorities, we have a very high bar for software quality, testability, and maintainability.</p> <p><b> Software and Data Infrastructure Team</b></p> <p> The SDI Team is a key enabler of the May Mobility mission, operationalizing the experience of using autonomous technology for real people on real routes. We leverage automation and DevOps culture to build cloud infrastructure and developer productivity tools at scale. We provide the data backbone of the company, from raw log data on vehicles to consumers with varying needs. We build experiences for internal teams managing the fleet, external customers and passengers. Members of this tight-knit group act as a force multiplier for the company.</p> <p><b> Your Opportunity to Drive Success</b></p> <ul> <li>Build state-of-art data distribution, storage and analysis platforms powering experiences for internal and external customers</li> <li>Manage and scale our real-time and historical data pipelines to enable our fleet to operate and facilitate continuous development of our system</li> <li>Contribute to designing and implement data models for optimal storage and retrieval meeting requirements of stakeholders with different needs</li> <li>Define, build, and expand libraries and APIs for managing, searching, and analyzing vehicle datasets with internal and external partners</li> <li>Participate in new technology introduction initiatives for modern data tools and industry best practices</li> </ul> </div>",
        "requirement_text": "<div> <ul> <li>B.S. Degree in Computer Science, Computer Engineering, or an equivalent degree and 2+ years of industry experience</li> <li>Hands-on experience with distributed technology such as Kafka, Spark, Spark Streaming, Storm, Flink, Cassandra</li> <li>Strong working knowledge of data structures and algorithms</li> <li>Experience in an object oriented programming language, such as C++, Python, or Java</li> <li>Attention to detail and rigorous testing methodology</li> <li>Written and verbal communication skills</li> <li>Experience with robotics, automotive engineering, or start-ups is not required</li> <li>Ability to undergo a driving record check</li> </ul> <ul> <li>M.S. Degree in Computer Science, Computer Engineering and 2+ years of industry experience</li> <li>Expertise in Python, C/C++ or Java</li> <li>Experience building and managing large-scale data-processing pipelines in a cloud environment</li> <li>Working knowledge of telemetry systems and real-time data processing</li> </ul> <ul> <li>Competitive salary and benefits (medical / dental / vision / 401k)</li> <li>Meaningful stock incentives and equity refresh program</li> <li>Unlimited vacation / company paid holidays</li> <li>Daily catered lunches and snacks</li> <li>Paid parental leave</li></ul></div>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div><div><div>Location</div> <div>Remote</div> </div><div><div>Position Type</div> <div>Full Time</div> </div><div><div>Team</div> <div>Software engineering</div> </div><p>We’re Mitek, a NASDAQ-listed global leader in mobile capture and digital identity verification solutions built on the latest advancements in AI and machine learning. Our Mobile Verify and Mobile Deposit products power and protect millions of identity evaluations and mobile deposits every day, around the world.</p><p> Mitek is committed to the health and safety of our employees and candidates during the current pandemic. Our global team of Mitekians are successfully and productively working full-time from home. Your experience with us - from introduction, to interview, to onboarding - will be a virtual one.</p><p> Mitek is seeking a Data Engineer to join our US Data Science team. As the analytic muscle behind our large scale, globally distributed Digital Identity Verification cloud platforms, our technology teams rely on us to deliver large quantities of structured, semi-structured and unstructured data that enable them to build new capabilities and derive insights that drive all aspects of our platform delivery. Our goal is to continually build and implement solutions that support the operationalization of data acquisition and pipelining.</p><p> As a Data Engineer at Mitek, you'll join us in continually building and implementing solutions that support the operationalization of data acquisition and pipelining. The data solutions you create will directly impact our machine learning capabilities and global document coverage. We're a big data environment, and you'll be building data sets, working with data sources and data pipelines, and potentially even owning our data modeling. You'll work directly with our internal leaders in our Engineering, Product, R&amp;D, and Testing groups to support and ensure our teams can measure the performance and success of their efforts.</p><p> You'll need to have strong skills in Python development, as well as SQL. Experience in a big data environment like Hadoop is critical. And if you know some Golang, that'll be helpful too!</p> </div>",
        "requirement_text": "<div> <ul><li> Bachelor's degree in Mathematics, Statistics, Computer Science, or a related discipline</li><li> 3+ years of experience in data engineering or software engineering </li><li>Experience using Python or Go/Golang to perform scripting/development, data management, and data manipulation</li><li> Experience working with datasets used in the delivery of machine learning-based solutions</li><li> Experience with distributed messaging and streaming technologies (RabbitMQ, Kinesis, Kafka)</li><li> Experience creating dashboards and effective data visualization</li><li> Training and experience in statistics, data manipulation, visualization, and analysis</li><li> Successful history of creating and providing documentation to convey information and drive decision making</li></ul> <ul><li> Knowledge of data mining, machine learning, natural language processing, or information retrieval</li><li> 2-4 years of experience in a quantitative role</li><li> Experience using Tableau as a tool for the development of data analytics and decision support solutions</li><li> Experience deploying software to a cloud platform environment. AWS, GCP, Azure.</li><li> Exposure to Big Data platforms and technologies</li><li> Exposure to SQL and NoSQL databases and document stores such as MySQL, Aurora, RedShift, MongoDB, RavenDB etc.</li><li> Experience processing large amounts of structured and unstructured data</li><li> Prior experience in secure practices of handling sensitive data and PII</li></ul></div>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div><div><b>SocialChorus </b>is a platform for communicators. We help them become heroes within their organizations by giving them the tools and expertise they need to unify their enterprises. Companies thrive and win when all of their workers feel aligned, informed and supported. In simple terms, we help companies work as one.</div><div></div><br> <div> Joining SocialChorus means joining a movement to connect every worker. This movement has taken root and is evident in our world-class customer base and their millions of employees worldwide. Now we need your help to achieve our goal of connecting every worker. Ready to make a difference?</div><div></div><br> <div> We are currently seeking a <b>Data Engineer</b> to help elevate our communications platform which is being used by the largest companies in the world in some of the most technically complex environments you can find. You will be helping in implementation of “big data” systems which require queries returning within sub-second response times. Ready for a challenge?</div><div></div><br> <div> We are a distributed team. We build solutions for distributed workforces so we model our workforce the same way. In this role you really can work where you want, but for this role we are only considering candidates based and authorized to work in the United States </div></div>",
        "requirement_text": "<div> <div><div><div><ul><li> 3- 5 years of experience.</li><li> Experience with large-scale data and query optimization techniques.</li><li> Experience with ETL to data warehouse systems.</li><li> Experience with AWS cloud services: EC2, RDS, Redshift, Aurora Postgres.</li><li> Proficient in SQL</li><li> Understanding of NoSQL and RDBMS.</li><li> Knowledge in multiple scripting languages (e.g. Python).</li><li> Knowledge of cloud, distributed systems, and stream-processing systems.</li><li> Passionate about learning new technologies and solving hard problems in a fast-paced environment.</li></ul></div></div></div> <div><div><div><ul><li> Has a Computer Science degree.</li><li> Has 2+ years experience in SaaS development environments.</li><li> Enjoys learning from teammates, and isn't afraid to teach others at the same timeSees the glass half-full. </li><li>This is a new industry space...your vision could make all the difference!</li><li> Wants to make a lasting impact and lifelong connections, this is not just another paycheck</li></ul></div></div></div> </div>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div><div><p><b>About Lokavant</b></p> <p> Lokavant is a technology company whose mission is to ensure that no clinical trial fails due to operational error. By integrating and analyzing the disparate data sources within clinical trials, Lokavant provides real-time visualizations and risk alerts to study sponsors and contract research organizations (CROs) to enable data-driven decisions. These insights expedite trial timelines and reduce the costs of development, allowing safe and efficacious treatments to get to patients.</p> <p> Lokavant centralizes trial data to power a machine learning model that anticipates trial risk, provides data-driven risk mitigation strategies, and predicts the impact of mitigation strategy implementation. Lokavant's anticipatory monitoring capability is grounded in a compendium of data from over 1,000 clinical trials and will improve with each deployment.</p></div><p><b> About the Opportunity</b></p> <p> How often are you given the opportunity to build something from the ground up, with an abundance of resources at your disposal; to be part of a team of people accomplished in diverse scientific and engineering disciplines, focused on using the best of what lies at the forefront of technology to address complex, real-world problems that have a positive impact on potentially millions of peoples' lives? This is that kind of opportunity.</p> <p> We are seeking a thoughtful, hands-on technology enthusiast with a strong aptitude for data engineering to join the rapidly growing Lokavant team in our New York City headquarters. The Data Engineer will work very closely with our front-end developers, back-end developers, development operations engineers, and data scientists. Our platform is fully cloud-based and is being built around modern tools and frameworks in an incredibly fast-moving agile environment.</p><br> <p></p> <p><b> Key Responsibilities</b></p> <ul> <li>Design, develop, and implement data infrastructure and pipelines that ingest and transform data from various external sources, storing it in highly optimized database systems, and making it useful to our application and reporting layers</li> <li>Create automation systems and tools to configure, monitor, and orchestrate data infrastructure and pipelines</li> <li>Create data integration services to help onboard new customers as quickly as possible</li> <li>Maintain ongoing reliability, performance, and support of the data infrastructure, providing solutions based on application needs and anticipated growth</li> <li>Participate in creating and maintaining strict compliance, data privacy and security measures</li> <li>Develop robust and production-level code to implement new product features in collaboration with other engineers and subject matter experts</li> <li>Identify and resolve performance and scalability issues, troubleshoot problems, and improve product quality</li> <li>Collaborate with the Front-End Development team to thread the right information through to forward-facing applications</li> <li>Interface with the Development Operations colleagues to evaluate and implement methodologies and workflows to facilitate the frequent and continuous release of high-quality software</li> <li>Work closely with Data Science colleagues to implement descriptive and predictive algorithms and models using the latest technologies</li> <li>Keep up to date on emerging technology solutions, particularly those on AWS, for continuous improvements in data engineering</li> <li>Help recruit highly capable engineers to the team from diverse backgrounds</li> <li>Mentor and be mentored by engineers of varied experience levels and subject matter areas</li></ul><br> <p></p> </div>",
        "requirement_text": "<ul> <li>3+ years relevant experience with data engineering</li> <li>Strong proficiency with Python (ideally PySpark) and SQL</li> <li>Experience with AWS S3, EC2, EMR, or an equivalent cloud-hosted infrastructure</li> <li>Experience with cloud-hosted database/data warehouse architecture (e.g. Redshift, Snowflake, etc.)</li> <li>Experience writing and productionizing complex data transformations in SQL and related frameworks</li> <li>Interest in building distributed computing and orchestration frameworks (e.g. Spark, Kubernetes, Airflow, etc.)</li> <li>Experience working in an Agile software development environment</li> <li>Exceptional written and verbal communication skills</li> <li>Strong attention to detail and highly organized, with effective multi-tasking and prioritization skills</li> <li>Proactive, self-motivated and self-directed, with the ability to learn quickly and autonomously</li> <li>Comfortable with ambiguity</li> <li>Superior problem-solving and troubleshooting skills</li> <li>Ability to work as part of a collaborative cross-functional team in a fast-paced environment</li> <li>Sincere interest in working at a rapidly changing start-up and scaling with the company as we grow</li> <li>Bachelor's degree with strong academic performance in Computer Science, Software Engineering, Applied Science, or equivalent field</li></ul>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div><p>Data Engineers at Red Ventures drive business value by transferring and structuring data for production and consumption by engineers, businesses, analysts, data scientists and partners. You will join a team of highly skilled engineers who design, develop and automate high-quality, scalable solutions across the entire data lifecycle, from raw data to powerful insights and analytics. We are using Spark as our universal program on how we are transferring data across multiple platforms. If you want to join a team of collaborative, passionate and adept engineers that share your passion for high quality; this is the role for you.</p> <p><b> What You'll Be Doing:</b></p> <ul> <li>Building large scale, real time data pipelines with Spark/Scala for Media and Technology data team, serving businesses like CNET.com, GameSpot.com, ZDNet.com, Time,Inc-NextAdvisor, etc.</li> <li>Working with a cross functional team of data scientists and analysts to understand business requirements leveraging our data.</li> <li>Design scalable solutions across distributed systems utilizing cutting edge cloud and big data technologies.</li> </ul> </div>",
        "requirement_text": "<ul> <li>5 years of experience in software development, data engineering, business intelligence, data science or related fields with good exposure in distributed systems, distributed data processing frameworks, data warehouses, technical architectures, infrastructure components, ETL/ELT design patterns.</li> <li>2 years of experience with SQL relational database (examples include Oracle, SQL Server, Postgres, MySQL, Teradata)</li> <li>2 years of experience with data processing (Hadoop, Spark), working in RDDs and DataFrames/Datasets API (with emphasis on DataFrames) to query and perform data manipulation</li> <li>2 years of experience with any cloud computing platforms, preferably AWS (Kinesis, S3, Lambda, DynamoDB, Redshift)</li> <li>Experience building large scale Spark applications</li> <li>Knowledge of Scala is preferred (Java, Go and Python is also acceptable)</li> <li>Experience in SparkSQL (Broadcast Joins)</li> <li>Experience in data engineering solutions best practices, including coding standards, code reviews, source control management (GitHub), build processes (CI/CD), testing and operations</li> <li>Linux common working knowledge, including navigating through the file system and simple bash scripting is a plus</li> <li>Experience with Storm or Cassandra is a plus</li> <li>Experience in Spark Structured Streaming is a plus</li> <li>Knowledge about agile software processes is a plus</li> </ul>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div><div><b>CBO – Systems Engineering – Project Delivery Specialist – AWS Data Engineer - TPP</b></div><div> Are you an experienced, passionate pioneer in technology – a solutions builder, a roll-up-your-sleeves technologist who wants a daily collaborative environment, think-tank feel and share new ideas with your colleagues - without the extensive demands of travel? If so, consider a remote opportunity with us.<br> <br> <br> <br> <b>Work you’ll do</b><br> As an AWS Data Engineer, you will be expected to perform the following: <br> <ul><li>Creating/managing AWS services.</li><li>Work with distributed systems as it pertains to data storage and computing.</li><li>Building and supporting real-time data pipelines</li><li>Design and build of data extraction, transformation, and loading processes by writing Step functions or custom data pipelines.</li></ul></div><div></div><br> </div>",
        "requirement_text": "<ul><li>Must have significant experience with AWS data services</li><li>Strong database experience in Relational, Columnar, NOSQL &amp; Timeseries databases.</li><li>Working experience on building and supporting real-time data pipelines using AWS Glue, Redshift/Spectrum, Kinesis, Firehose, Pyspark, EMR and Athena.</li><li>Knowledge and hands-on experience with AWS solutions including S3, SNS, SQS, DynamoDB, Redshift and AWS RDS.</li><li>Experience in the design and build of data extraction, transformation, and loading processes by writing Step functions or custom data pipelines.</li><li>Nice to have experience working on Hadoop, Data Bricks</li><li>Familiarity with log formats from various AWS services such as S3 server access , CloudFront distribution, Lambda execution, ELB, Container execution etc.</li><li>Experience on creating AWS Lambda functions using Python or R scripts.</li><li>Familiarity with AWS infrastructure related services such as: AWS VPC, EC2 Instances, Network policies and Cloud Watch.</li></ul>",
        "job_title": "AWS Data Engineer"
    },
    {
        "description_text": "<div><p>As a <b><i>Data Engineer</i></b>, you will be responsible for designing, managing, and optimizing data processes for Blue Corona’s platforms. We set our clients’ expectations that our recommendations and strategies are properly supported by data, testing, and experience; your role is to ensure that our team is empowered with data to do so. You will work with our marketing technology stack and provide technical guidance for taking our data from acquisition through visualization and application to enable Blue Corona’s data-driven approach.</p><p><b> Responsibilities</b></p><ul><li> Understand and translate business requirements, both internal and external, for Blue Corona’s platforms</li><li> Design, create, and manage ETL pipelines across a variety of data collection technologies and platforms</li><li> Identify, build, and extend analytics tools that utilize the data pipeline to provide actionable insights into ways to improve data reliability, efficiency, and quality</li><li> Identify, design, and implement internal process and improvements: automating manual processes, optimizing data acquisition and delivery, re-designing infrastructure for greater scalability, etc.</li><li> Document and communicate platform updates and upgrades to Blue Corona team</li></ul></div>",
        "requirement_text": "<div><ul><li> Bachelor’s degree in Information Systems, Computer Science, Informatics or a related field</li><li> 4 years of experience as a Data Engineer or similar role</li><li> Experience building and optimizing data pipelines</li><li> Experience programming with SQL, and relational databases such as MySQL</li><li> Experience building web applications with Python, PHP, and other programming languages, with an emphasis on writing clean, efficient, and scalable code</li><li> Experience with parsing data formats such as XML/JSON and leveraging external APIs</li><li> Experience with AWS cloud-based infrastructure (including EC2, RDS, and S3)</li><li> Experience with DevOps tools like Git/GitHub</li></ul><ul><li> Experience with Django framework</li><li> Familiarity with digital marketing measurement and analytics platforms and principles</li></ul></div>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div><p>Are you looking for a friendly, fast-paced workplace with an emphasis on helping customers and empowering team members? Snap Finance is a thriving leader in the financial services industry, and our team members are the foundation of our success. Snap knows that happy, empowered, and engaged team members are essential to innovation and business success- and our approach is working. Come join us!</p> <p> Become part of an amazing Analytics team at Snap! We are looking for a Data Engineer to help us design and maintain scalable, secure, and reliable data solutions across our operational and analytics systems. This will involve building pipelines to integrate data sources using a variety of languages and tools. The ideal candidate is eager to learn and driven toward finding excellent solutions to complex problems. The Data Engineer will support our developers, data scientists, business intelligence analysts, and machine learning engineers in ensuring consistent, accurate data delivery.</p> </div>",
        "requirement_text": "<ul> <li>Streaming data systems such as Kafka or Kinesis</li> <li>Distributed processing using tools such as Spark and Flink</li> <li>Message queuing systems such as RabbitMQ</li> <li>Distributed database and caching systems such as Citus, CockroachDB, Redis, memcached, Alluxio</li> <li>Column-oriented data formats such as Parquet, ORC</li> <li>Automated workflows and CI/CD tools: Airflow, Argo, Jenkins, etc.</li> <li>Container-based deployments using Docker and Kubernetes</li> <li>Amazon Web Services: S3, EC2, ECR, ELB, RDS, DynamoDB, etc.</li> <li>Declarative infrastructure using tools such as Terraform</li> </ul>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div><p><b>About Bevy</b></p> <p> Bevy is an early stage Startup with a mission to help brands build strong global communities. Founded in April 2017 by the core team behind Startup Grind, Bevy is an enterprise-grade SaaS platform used by companies that include Adobe, Amazon, Asana, Atlassian, Ebay, Epic Games, IDEO, Intuit, MongoDB, Red Bull, Roblox, Salesforce, SAP, Slack and many more. In April 2019, Bevy acquired CMX which is the world's largest network of community professionals. CMX offers world-class training, events and research.</p> <p><b> The Role</b></p> <p> As Bevy's first dedicated data engineer, you will be embedded in Bevy's data analytics and insights team. You will help guide data architecture and system setup and support continuous improvement to the organizations ability to store and process data.</p> <p><b> Responsibilities</b></p> <ul> <li>Create and maintain testable ETL pipelines using GCP services and Python</li> <li>Optimize data ingestion, storage, and processing architecture to meet product, business, and performance needs</li> <li>Care deeply about data quality, privacy, and security</li> <li>Support data scientists and product engineers</li> <li>Proactively support continuous learning and software engineering projects</li> <li>Examine and troubleshoot data stored in SQL, NoSQL</li> <li>Support data/analytics application development and R&amp;D efforts</li> <li>Proactively support continuous learning and software engineering projects</li> </ul> </div>",
        "requirement_text": "<ul><li>Bachelor's degree, or substantial coursework in Computer Science, Software Engineering, or similar</li> <li>Familiarity with Hadoop ecosystem tools</li> <li>Experience creating product data pipelines on GCP using Pub/Sub, Dataflow, and BigQuery, and Python</li> <li>Experience supporting ML projects</li> <li>You like solving interesting, hard problems and communicating the results in accessible ways</li> <li>You ask probing questions about software and don't give up easily</li> <li>You reside in North or South America. Yes, we are a distributed company, but since we are still small, we like to minimize the time zone spread within the team</li> <li>You are an excellent communicator. In our small team, English is the official language. You need to be able to articulate complex ideas efficiently and effectively. When people do not share an office, it is essential to pay extra attention to communication &amp; speak up.</li> </ul>",
        "job_title": "Data Engineer"
    },
    {
        "description_text": "<div><p><b>Who we are</b></p><p></p><p> SoFi is a digital personal finance company whose mission is to help its members achieve financial independence to realize their ambitions, whether that be to buy a house one day, start a family on their own terms or be debt free. We aim to be at the center of our members’ financial lives, and to help every member Get Their Money Right®. By joining SoFi, you’ll become part of a forward-thinking company that is transforming financial services by embracing technology to build innovative loan products, investment tools, and more. One of the fastest growing fintech companies, we’ve grown from 250 employees in 2015 to over 1,500 employees today, with over 1 million members. With offices across the US, we offer the excitement of a rapidly growing startup with the stability of a seasoned management team and some of the best talent around. As an employer, we strive to hire employees who are committed to both our company’s mission and our desire to build the best culture in the world. If you are driven, passionate about what you do, and excited about the SoFi mission, we would love to hear from you.</p><p></p><p><b> The role</b></p><p></p><p> SoFi runs on data! In this role you will be contributing to the long-term success of SoFi’s big data vision of establishing a democratized data platform that enables teams to ingest, model, and consume data with confidence. Join the Big Data Infrastructure team as it refines this vision and establishes industry-leading standards for data lifecycle management ushering in best-in-class architectural components and processes in extracting value from disparate data sources. The success of this team is central to the success of the company and your contributions will have very visible and lasting impact.</p><p></p><p> As an engineer on the big data platform at SoFi, you'll be tasked with building critical components and features. You will implement battle-tested patterns and interfaces, squash bugs, refactor code and continually grow as an engineer. The ideal candidate has a strong software engineering background and problem-solving ability. Additionally, you will demonstrate SoFi’s core values by honing your skills as an effective communicator, showing personal responsibility, and setting ambitious goals. If you like working on problems with tangible and lasting impact, we would love to have you in our team!</p><p></p><p><b> What you’ll do:</b></p><ul><li><p> Directly contribute to high-performance batch and stream data processing systems</p></li><li><p> Manage and evolve cloud-based data warehousing services</p></li><li><p> Work with amazing product and business managers to deliver compelling features</p></li><li><p> Partner with team members to implement and design interfaces and abstractions</p></li><li><p> Sharpen your skills as a developer and build technical domain knowledge on data infrastructure modernization</p></li></ul></div>",
        "requirement_text": "<div><ul><li><p> Bachelor’s degree, ideally in a technical field</p></li><li><p> 2+ years experience as a software Engineer</p></li><li><p> Intellectual curiosity and aptitude to pick up new technical skills</p></li><li><p> Skilled at reading and understanding technical documentation</p></li><li><p> Ability to focus on tasks and drive work to completion</p></li><li><p> A passion and instinct for data quality</p></li><li><p> Ability to influence outcomes and discuss technical challenges with team members</p></li><li><p> Strong fundamentals of data structures, algorithms, and design patterns</p></li><li><p> Software development experience in Java, C/C++, or C#</p></li><li><p> Experience building solutions using public clouds (Azure, AWS, GCP)</p></li><li><p> Proficiency with SQL and strong Python development skills</p></li><li><p> Familiarity with big data platforms and tooling (AWS, Snowflake, Kafka, Luigi, Hadoop, Hive, Spark, Cassandra, Airflow, etc).</p></li></ul><ul><li><p> Prior experience with CI/CD (gradle, git, automated testing and deployments)</p></li><li><p> Data exploration and analysis experience using SQL/Python/R/Tableau. Experience surfacing insights using math/statistics/ML techniques</p></li><li><p> Passion and curiosity for FinTech</p></li></ul></div>",
        "job_title": "Data Engineer"
    }
]